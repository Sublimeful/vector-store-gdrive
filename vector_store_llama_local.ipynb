{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4762c44b",
      "metadata": {
        "id": "4762c44b"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/vector_stores/SimpleIndexDemoLlama-Local.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c48213d-6e6a-4c10-838a-2a7c710c3a05",
      "metadata": {
        "id": "9c48213d-6e6a-4c10-838a-2a7c710c3a05"
      },
      "source": [
        "# Local Llama2 + VectorStoreIndex\n",
        "\n",
        "This notebook walks through the proper setup to use llama-2 with LlamaIndex locally. Note that you need a decent GPU to run this notebook, ideally an A100 with at least 40GB of memory.\n",
        "\n",
        "Specifically, we look at using a vector store index."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91f09a23",
      "metadata": {
        "id": "91f09a23"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfe5fbf2",
      "metadata": {
        "id": "dfe5fbf2",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%pip install llama-index-llms-huggingface\n",
        "%pip install llama-index-embeddings-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73e14011",
      "metadata": {
        "id": "73e14011",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install llama-index ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-readers-google"
      ],
      "metadata": {
        "id": "gPmCuIlai252",
        "collapsed": true
      },
      "id": "gPmCuIlai252",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip reinstall torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
      ],
      "metadata": {
        "id": "xm6tg81yguuJ",
        "collapsed": true
      },
      "id": "xm6tg81yguuJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade accelerate\n",
        "!pip install --upgrade -i https://pypi.org/simple/ bitsandbytes"
      ],
      "metadata": {
        "id": "_sNDm4c9hl0e"
      },
      "id": "_sNDm4c9hl0e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "50d3b817-b70e-4667-be4f-d3a0fe4bd119",
      "metadata": {
        "id": "50d3b817-b70e-4667-be4f-d3a0fe4bd119"
      },
      "source": [
        "### Set Up"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9073233e",
      "metadata": {
        "id": "9073233e"
      },
      "source": [
        "**IMPORTANT**: Please sign in to HF hub with an account that has access to the llama2 models, using `huggingface-cli login` in your console. For more details, please see: https://ai.meta.com/resources/models-and-libraries/llama-downloads/."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "82YZFvNjhMPb"
      },
      "id": "82YZFvNjhMPb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "690a6918-7c75-4f95-9ccc-d2c4a1fe00d7",
      "metadata": {
        "id": "690a6918-7c75-4f95-9ccc-d2c4a1fe00d7"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
        "\n",
        "\n",
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be92665d",
      "metadata": {
        "id": "be92665d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.core import PromptTemplate\n",
        "\n",
        "# Model names (make sure you have access on HF)\n",
        "LLAMA2_7B = \"meta-llama/Llama-2-7b-hf\"\n",
        "LLAMA2_7B_CHAT = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "LLAMA2_13B = \"meta-llama/Llama-2-13b-hf\"\n",
        "LLAMA2_13B_CHAT = \"meta-llama/Llama-2-13b-chat-hf\"\n",
        "LLAMA2_70B = \"meta-llama/Llama-2-70b-hf\"\n",
        "LLAMA2_70B_CHAT = \"meta-llama/Llama-2-70b-chat-hf\"\n",
        "\n",
        "selected_model = LLAMA2_7B_CHAT\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"You are an AI assistant that answers questions in a friendly manner, based on the given source documents. Here are some rules you always follow:\n",
        "- Generate human readable output, avoid creating output with gibberish text.\n",
        "- Generate only the requested output, don't include any other language before or after the requested output.\n",
        "- Never say thank you, that you are happy to help, that you are an AI agent, etc. Just answer directly.\n",
        "- Generate professional language typically used in business documents in North America.\n",
        "- Never generate offensive or foul language.\n",
        "\"\"\"\n",
        "\n",
        "query_wrapper_prompt = PromptTemplate(\n",
        "    \"[INST]<<SYS>>\\n\" + SYSTEM_PROMPT + \"<</SYS>>\\n\\n{query_str}[/INST] \"\n",
        ")\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    context_window=4096,\n",
        "    max_new_tokens=2048,\n",
        "    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
        "    query_wrapper_prompt=query_wrapper_prompt,\n",
        "    tokenizer_name=selected_model,\n",
        "    model_name=selected_model,\n",
        "    device_map=\"auto\",\n",
        "    # change these settings below depending on your GPU\n",
        "    model_kwargs={\"torch_dtype\": torch.float16, \"load_in_8bit\": True},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cd31d66",
      "metadata": {
        "id": "2cd31d66"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2f84fac",
      "metadata": {
        "id": "c2f84fac"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Settings\n",
        "\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "390490b1",
      "metadata": {
        "id": "390490b1"
      },
      "source": [
        "Download Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "mjp81QFhgGEd"
      },
      "id": "mjp81QFhgGEd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03d1691e-544b-454f-825b-5ee12f7faa8a",
      "metadata": {
        "id": "03d1691e-544b-454f-825b-5ee12f7faa8a"
      },
      "outputs": [],
      "source": [
        "from llama_index.readers.google import GoogleDocsReader\n",
        "\n",
        "document_ids = [\"1oMMvf2wEH6bVeXA0Pd_3ua8YQszLELXUx6SsnKCAqS0\"]\n",
        "# load documents\n",
        "documents = GoogleDocsReader().load_data(document_ids=document_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad144ee7-96da-4dd6-be00-fd6cf0c78e58",
      "metadata": {
        "id": "ad144ee7-96da-4dd6-be00-fd6cf0c78e58"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "index = VectorStoreIndex.from_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6caf93b-6345-4c65-a346-a95b0f1746c4",
      "metadata": {
        "id": "b6caf93b-6345-4c65-a346-a95b0f1746c4"
      },
      "source": [
        "## Querying"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85466fdf-93f3-4cb1-a5f9-0056a8245a6f",
      "metadata": {
        "id": "85466fdf-93f3-4cb1-a5f9-0056a8245a6f"
      },
      "outputs": [],
      "source": [
        "# set Logging to DEBUG for more detailed outputs\n",
        "query_engine = index.as_query_engine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdda1b2c-ae46-47cf-91d7-3153e8d0473b",
      "metadata": {
        "id": "bdda1b2c-ae46-47cf-91d7-3153e8d0473b"
      },
      "outputs": [],
      "source": [
        "response = query_engine.query(\"What did the author do growing up?\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24935a47",
      "metadata": {
        "id": "24935a47"
      },
      "source": [
        "### Streaming Support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "446406f9",
      "metadata": {
        "id": "446406f9"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "query_engine = index.as_query_engine(streaming=True)\n",
        "response = query_engine.query(\"What happened at interleaf?\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "token_count = 0\n",
        "for token in response.response_gen:\n",
        "    print(token, end=\"\")\n",
        "    token_count += 1\n",
        "\n",
        "time_elapsed = time.time() - start_time\n",
        "tokens_per_second = token_count / time_elapsed\n",
        "\n",
        "print(f\"\\n\\nStreamed output at {tokens_per_second} tokens/s\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}